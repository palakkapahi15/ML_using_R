#LINEAR AND POLYNOMIAL REGRESSION
df <- read.csv("House_Price.csv")
View(df)
summary(df)
hist(df$crime_rate)
pairs(~price+crime_rate+n_hot_rooms+rainfall, data = df)
barplot(table(df$airport))
barplot(table(df$waterbody))

#Observations after performing EDD(Extended data dictionary) of each variable
#1. n_hot_rooms, rainfall as outliers(after analysing the summary of the df - diff
#between mean and median values as well as the distributions in quartiles of skewness and outliers)
#2. n_hos_beds as missing value(analysed in summary itself)
#3. bus_terminal is a useless variable(while plotting barplots for the categorical data)
#4. crime_rate has some other functional relationship with price(analysed during summary of df i.e
#plotting the histograms)

#Outlier treatment
#1. capping and flooring - (after ordering all the values in ascending order)upper limit value should be the 3*99percentile of the value and 
# lower value should be the 0.3*1percentile of the values....beyond these limits we will 
# change the values
#2. Exponential smoothening - extrapolate curve between P95 to P99 and and cap all the values falling outside
# to the value generated by the curve
# Similarly extrapolate the curve between P5 and P1
#3. Sigma Approach - All value beyond some sigma by 3*sd, they are replaced by mean + 3sd
# 1. and 3. are equivalent techniques

#Using capping and flooring technique

uv <- 3 *quantile(df$n_hot_rooms,0.99)
df$n_hot_rooms[df$n_hot_rooms > uv] <- uv
summary(df$n_hot_rooms)
lv <- 0.3 * quantile(df$rainfall,0.01)
lv
df$rainfall[df$rainfall < lv] <- lv
summary(df$rainfall)

#Missing value treatment
# Replace the mising values with the mean/mesdian values
#1. Impute with zero(in any particular scenario)
#2. Impute Mean/Median/Mode
#3. For categorical data we ll impute the Mode value
#4. Segment based imputation

#Missing Values in n_hos_beds

mean(df$n_hos_beds, na.rm = TRUE)
which(is.na(df$n_hos_beds))
df$n_hos_beds[is.na(df$n_hos_beds)] <- mean(df$n_hos_beds, na.rm = TRUE)
summary(df$n_hos_beds)
which(is.na(df$n_hos_beds))

# Seasonality data(repeating pattern varies due to seasons)
# To remove seasonality from the data we multiply the correction data to the data
# m = mean(year) / mean(month)

# Bivariate Analysis - Simultaneous analysis of the two varibles
#1. Using scatter plots
#2. Correlation

# Variable transformation
#1. Transforming similar multiple variables into a single variable by taking mean os all those
# similar variables
#2. Creating ration variables which are related to business
#3. Transforming variables by taking log(1+x), exponential(x), roots, squares, cubes, etc

plot(df$crime_rate,df$price)
df$crime_rate <- log(1+df$crime_rate)
plot(df$price,df$crime_rate)
plot(df$crime_rate, df$price)

df$avg_dist <- (df$dist1 + df$dist2 + df$dist3 + df$dist4)/4
View(df)
df <- df[,-7:-10]

df <- df[,-14]

#Non usable variables
#1. variables with single unique value
#2. variables with low fill rate
#3. variables with regularity issue
#4. variables with no business sense


# we can remove the rainfall variable
plot(df$rainfall, df$price)
df1 <- df[,-13]
View(df1)

#Dummy values(0,1) for string data for categorical data
installed.packages()
install.packages("devtools")
library(devtools)
install_url('https://cran.r-project.org/src/contrib/Archive/dummies/dummies_1.03.tar.gz')

df <- dummy.data.frame(df)
View(df)
install.packages("fastDummies")
library(fastDummies)
df <- dummy_cols(df,select_columns = "airport")
df <- dummy_cols(df,select_columns = "waterbody")
df <- df[,-9]
df <- df[,-11]
View(df)
df <- df[,-14]
df <- df[,-18]

#Corelation to find multicolinearity
cor(df)
round(cor(df),2)

#Firstly we'll check the correlation between other variables and price
#After that we'll check the corelation between other variables amongst each other instead of price
#If any of the pair has high colinearity(more than 0.8 or less than -0.8) between them....then we'll have to remove one of the variable amongst them in order to avoid multicolinarity
#We will remove that varible which has less correlation with price

df <- df[,-12]
View(df)

#Simple Linear regression
simple_model <- lm(price~room_num, data = df)
summary(simple_model)
plot(df$room_num,df$price)
abline(simple_model)

#Multiple Linear regression
multiple_model <- lm(price~., data = df)
summary(multiple_model)

#Using caTools we will split the data into train set and test set
library(caTools)
set.seed(0)
split <- sample.split(df, SplitRatio = 0.8)
split
training_set <- subset(df, split == TRUE)
training_set
test_set <- subset(df, split == FALSE)
test_set
lm_a <- lm(price ~., data = training_set)
summary(lm_a)
train_a <- predict(lm_a, data = training_set)
train_a
test_a <- predict(lm_a, data = test_set)
mean((training_set$price - train_a)^2)
mean((test_set$price - test_a)^2)

#Training model using subset selection method
install.packages("leaps")
library(leaps)
lm_best <- regsubsets(price~., data = df, nvmax = 15) 
summary(lm_best)
#Adjusted R^2 value
summary(lm_best)$adjr2
which.max(summary(lm_best)$adjr2)
coef(lm_best,8)

#Forward selection
lm_forward = regsubsets(price~., data = df, nvmax = 15, method = "forward")
summary(lm_forward)

#Backward selection
lm_backward = regsubsets(price~., data = df, nvmax = 15, method = "backward")
summary(lm_backward)

#Shrinkage methods
#1. Ridge Regression
install.packages("glmnet")
library(glmnet)
#x has independant variable
x <- model.matrix(price~.,data = df)[,-1]
#y has dependant variable
y <- df$price
#To find lambda which has min error
grid = 10^seq(10,-2,length = 100)
grid
# alpha = 0 - Ridge regression and alpha = 1 - Lasso Regression 
lm_ridge <- glmnet(x,y,alpha = 0, lambda = grid)
summary(lm_ridge)
#To find optimum value of lambda
#Library glmnet has inbuilt function for cross validation
cv_fit <- cv.glmnet(x,y,alpha = 0,lambda = grid) 
plot(cv_fit)

opt_lambda <- cv_fit$lambda.min
opt_lambda

#total sum of squares
total_sum_of_squares <- sum((y - mean(y))^2)
#Predicted values
y_a <- predict(lm_ridge,s = opt_lambda, newx = x)
#residual sum of squares
rss <- sum((y_a - y)^2)

rsq <- 1-rss/total_sum_of_squares

#Lasso Regression
lm_lasso <- glmnet(x,y,alpha = 1, lambda = grid)
#rest steps are similar to ridge regression

#--------------------------------------------------------------------------------

#Three classification models -
#1. Logistic Regression
#2. Linear Discrimination Analysis(LDA)
#3. K-Nearest neighbors(KNN)

#Regression VS Classification
#Target variable/Dependent variable is continuous -> Regression
#Target variable/Dependent variable is categorical -> Classification

#NOTE:Logistic Regression is a classification model and not a regression model

#LOGISTIC REGRESSION
#Now we want to find the selling potential of the property within 3 months of getting listed(sold varibale will be the dependent variabele)

df <- read.csv("Classification_data.csv", header = TRUE)
View(df)
str(df)

#Problem Statement - 
#1. Prediction question
#2. Inferential question(accuracy of each predictor variable)

#Why cant we use Linear Regression for classifiaction problem?
#-> Linear Regression cannot be used for more than two categories(cannot have more than 2 dummy variable)
#-> Predicted values are not that reliable as probability value is not between 0 and 1
#-> If line has outline points then the line tends to change its slope 

#Logistic function/Sigmoid function
#In Linear Regression model we use OLS(Ordinary Least Squares method)
#In Logistic Regression model we use Maximum Likelihood method
#When a person is defaulting, the default value should be maximising to 1, and with non defaukting, the value should be maximising to 0
#Logistic Regression can be done in multiple classes

#Logistic Regression with simple predictor
glm.fit <- glm(Sold~price,data = df, family = binomial)
summary(glm.fit)

#Logistic regression with multiple predictors
sglm.fit <- glm(Sold~.,data = df, family = binomial)
summary(glm.fit)

#Confusion Matrix - Which tells us the performance of our model
#columns - True values(Yes or NO,True or False,Defaulted or not defaulted)
#Rows - Predicted values
#Type 1 error - Predict "Yes" but actual is "NO"
#Type 2 error - Predict "No" but actual is "Yes"

#Performance Measures
#1. False positive rate - Type I error, 1-Specificity
#2. True positive rate -  1-Type II error,power,sensitivity,recall
#3. Positive predictive value - Precision,1-false discovery proportion
#4. Negative Predictive value - Specificity
#5. Max Area under ROC(Receiver Operating Characteristics) curve - xaxis : False positive rate and yaxis: True positive rate

#Probability of houses that will be sold within 3 months or not
glm.prob <- predict(glm.fit, type = "response")
glm.prob[1:10]

#Using boundary condition we can assign classes to these values
glm.predict <- rep("No",506)
glm.predict[glm.prob > 0.5] <- "Yes"

#confusion matrix - to check the quality of our model
cm <- table(glm.predict,df$Sold)
cm

#LINEAR DISCRIMINANT ANALYSIS(LDA)
#-> This is preferred when response variables has more than two classes
#-> based on Bayes theorem
#-> conditional probabilities are calculated and acc to that classes are assigned 
#-> LDA prediction depends on normal distribution assumption
#-> input value of x is taken as it is, hence linear discriminant analysis

install.packages("MASS")
library(MASS)
lda.fit <- lda(Sold~.,data = df)
lda.fit
lda.pred <- predict(lda.fit,df)
lda.pred
View(lda.pred)
lda.pred$posterior

lda.class <- lda.pred$class
View(lda.class)

cm <- table(lda.class,df$Sold)
cm

sum(lda.pred$posterior[,1] > 0.8)

#cm quantifies the model;
#if we draw a cm on the trainig data(whole dataset) then it is called as training error.
#Now we will split the data into training set and test set.

#Splitting data techniques:
#1. Validation set approach- 80:20
#2. Leave one out cross validation
#3. K-Fold validation

library(caTools)
set.seed(0)
split <- sample.split(df,SplitRatio = 0.8)
training_set <- subset(df,split == TRUE)
test_set <- subset(df,split == FALSE)

train_fit <- glm(Sold~.,data = training_set, family = binomial)
test_prob <- predict(train_fit, test_set, type = "response")
test_prec <- rep("No",120)
test_prec[test_prob > 0.5] <- "Yes"

cm  <- table(test_prec, test_set$Sold)
cm

lda.fit <- lda(Sold~.,data = training_set)
lda.fit
lda.pred <- predict(lda.fit,test_set)
lda.pred
View(lda.pred)
lda.pred$posterior

lda.class <- lda.pred$class
View(lda.class)

cm <- table(lda.class,test_set$Sold)
cm

#K-Nearest Neighbors Classifier
library(class)
train_x <- training_set[,-16]
test_x <- test_set[,-16]
train_y <- training_set$Sold
test_y <- test_set$Sold

k = 1

#For standardization we do scaling
train_x_scaling <- scale(train_x)
test_x_scaling <- scale(test_x)

set.seed(0)

knn_pred <- knn(train_x_scaling,test_x_scaling,train_y,k = k)

cm <- table(knn_pred,test_y)
cm

#Results of Logistic Regression
#1. P values - represents the confidence value - inversely proportional
#2. Estimated value - beta values - represents the impact of independent variables - directly proportional also the signs have their impact with inversally relation
#3. Accuracy of 65%
#4. Performs well with linear boundary 

#Results in LDA(Linear Discriminant Analysis)
#1. Linear Discriminant Coefficients - similar to Estimated values(beta values)
#2. Accuracy of 66.6%
#3. Performs well with linear boundary
#4. If the assumption of continuous variables that are normally distributed is TRUE then LDA performs better than Logistic REgression

#Results in KNN(K - Nearest Neighbors)
#1. No Beta values are not available in KNN as it is non parametric(No functional relationship)
# therefor it is difficult to estimate the affect of individual independent variables(predictor variables) on the dependent variable(response variable)
#2. Performs well with non linear boundary
# Accuracy of 55%

#Steps - 
#1. Data collecetion

#2. Data Pre-procession
#-> Outlier Treatment
#-> Missing value imputations
#-> Variable Transformation

#3. Model training
#-> test-train split
#-> use template to train
#-> do iterations
#-> compare performance of different models using test set

#4. Select the best model 
#-> For prediction purposes choose model with best accuracy
#-> For interpretation purposes look at the coefficient values of parametric models

---------------------------------------------------------------------------------------
  
#Decision trees
#Types :- 
#1. Regression Tree - For continuous quantitative target variable
#2. Classification Tree - For discrete categorical target variable

#Regression Tree
#Steps - 
#1. Divide the predictor space st the values should be non-overlapping
#2. Goal is to minimize RSS(Sum of squared error)
#3. Split is based on RSS value
#4. Top down greedy approach is used(recursive binary splitting)
#5. Continue splitting till stopping criteria is reached

#Stopping criteria methods :-
#1. Minimum observations at internal nodes
#2. Minimum observations at leaf nodes
#3. Maximum depth of tree

#Importing database
library(readr)
Movie <- read_csv("Movie_regression.csv")
View(Movie)

#Data Preprocessing
summary(Movie)
#Observations-
#1. Time taken column has NA values
Movie$Time_taken[is.na(Movie$Time_taken)] <- mean(Movie$Time_taken,na.rm = TRUE)

#Test - train split
library(caTools)
set.seed(0)
split <- sample.split(Movie,SplitRatio = 0.75)
train <- subset(Movie, split == TRUE)
test <- subset(Movie, split == FALSE)

#Regression Tree
install.packages("rpart")
install.packages("rpart.plot")
library("rpart")
library("rpart.plot")

regtree <- rpart(Collection~.,data = train,control = rpart.control(maxdepth = 3))

rpart.plot(regtree,test,)